<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- This templete is borrowed from AR-NET  -->

<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="assets/favicon.ico">
    <link rel="icon" href="assets/favicon.ico">

    <style type="text/css">
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
            font-weight: 300;
            letter-spacing: 0.1px;
            font-size: 18px;
            margin-left: auto;
            margin-right: auto;
            width: 1100px;
        }

        video.header-vid {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px;
            -moz-border-radius: 10px;
            -webkit-border-radius: 10px;
        }

        a:link,
        a:visited {
            color: #1367a7;
            text-decoration: none;
        }

        a:hover {
            color: #208799;
        }

        hr {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }

        .title {
            margin-top: 50px;
            font-size: 40px;
            font-weight: bold;
            color: #182B49;
        }

        .authors {
            margin-top: 30px;
        }

        .institutions {
            margin-top: 5px;
        }

        .entities_list td {
            padding-left: 20px;
            padding-right: 20px;
            font-size: 20px;
        }

        .logos {
            width: 900px;
            margin-top: 10px;
        }

        .logos td {
            padding-left: 20px;
            padding-right: 20px;
        }

        .links {
            margin-top: 20px;
            font-size: 22px;
            width: 900px;
        }

        .qualitative_teaser {
            margin-top: 10px;
            margin-bottom: 25px;
            width: 1100px;
        }

        .video,
        .results,
        .footer {
            margin-top: 30px;
            margin-bottom: 35px;
        }

        .abstract-title,
        .framework-overview-title,
        .our-approach-title,
        .results-title,
        .video-title {
            margin-top: 30px;
            font-size: 32px;
            color: #182B49;
            width: 700px;
        }

        .hlight {
            font-weight: bold;
            color: #182B49;
            display: inline-block;
            font-size: 18px;
        }

        .abstract {
            margin-top: 25px;
            margin-bottom: 35px;
            text-align: justify;
            width: 900px;
            font-size: 18px;
            line-height: 23px;
        }

        .framework-teaser-img,
        .our-approach-img {
            margin-top: 30px;
            width: 850px;
        }

        .framework-overview-note,
        .our-approach-note {
            margin-top: 20px;
            margin-bottom: 35px;
            text-align: justify;
            width: 900px;
            font-size: 18px;
            line-height: 23px;
        }

        .results-content {
            margin-top: 30px;
            display: block;
        }

        .results-note {
            width: 400px;
            display: inline-block;
            text-align: justify;
            vertical-align: top;
            font-size: 18px;
            line-height: 23px;
        }

        .results-img {
            width: 480px;
            display: inline-block;
            padding-left: 20px;
        }

        .video-content {
            margin-top: 30px;
        }

        .footer {
            margin-bottom: 100px;
        }

        .paperpreview-img {
            width: 300px;
        }

        .footer-content {
            width: 1100px;
            line-height: 23px;
        }

        .footer-text {
            width: 780px;
            padding-left: 20px;
            font-size: 20px;
            line-height: 30px;
        }

    </style>
    <title>Learning to Discover and Detect Objects</title>
    <meta property="og:title" content="Learning to Discover and Detect Objects">
    <meta property="og:description" content="Fomenko et al, Learning to Discover and Detect Objects. In NeurIPS, 2022.">
</head>

<body data-new-gr-c-s-check-loaded="14.974.0">

    <center class="title">
        Learning to Discover and Detect Objects
    </center>

    <table align="center" class="entities_list authors">
        <tbody>
            <tr>
                <td align="center">
                    <span>
                        <a href="https://github.com/vlfom/" target="_blank">Vladimir Fomenko</a>
                        <sup>1*</sup>
                    </span>
                </td>
                <td align="center">
                    <span>
                        <a href="https://dvl.in.tum.de/team/elezi/" target="_blank">Ismail Elezi</a>
                        <sup>2</sup>
                    </span>
                </td>
                <td align="center">
                    <span>
                        <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>
                        <sup>3, 4</sup>
                    </span>
                </td>
                <td align="center">
                    <span>
                        <a href="https://dvl.in.tum.de/team/lealtaixe/" target="_blank">Laura Leal-Taix√©</a>
                        <sup>2, 4</sup>
                    </span>
                </td>
                <td align="center">
                    <span>
                        <a href="https://dvl.in.tum.de/team/osep/" target="_blank">Aljo≈°a O≈°ep</a>
                        <sup>2, 3, 4</sup>
                    </span>
                </td>
            </tr>
        </tbody>
    </table>

    <table align="center" class="entities_list institutions">
        <tbody>
            <tr>
                <td>
                    <center><span>Microsoft Azure AI<sup> 1</sup></span></center>
                </td>
                <td>
                    <center><span>Technical University of Munich<sup> 2</sup></span></center>
                </td>
                <td>
                    <center><span>Carnegie Mellon University<sup> 3</sup></span></center>
                </td>
                <td>
                    <center><span>Argo AI<sup> 4</sup></span></center>
                </td>
            </tr>
        </tbody>
    </table>

    <table align="center" width="400px" style="margin-top: 10px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <center>
                        <span style="font-size:14px">* work done while at Technical University of Munich</span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>

    <table align="center" class="logos">
        <tbody>
            <tr>
                <td align="center">
                    <a href="https://nips.cc/Conferences/2022/" target="_blank">
                        <img alt="NeurIPS 2022" src="./assets/logos/neurips.png" width="170">
                    </a>
                </td>
                <td align="center">
                    <a href="https://dvl.in.tum.de/" target="_blank">
                        <img alt="TUM" src="./assets/logos/tum.png" width="130">
                    </a>
                </td>
                <td align="center">
                    <a href="http://www.cs.cmu.edu/~deva/" target="_blank">
                        <img alt="CMU" src="./assets/logos/cmu.png" width="60">
                    </a>
                </td>
                <td align="center">
                    <a href="https://www.microsoft.com/en-us/research/" target="_blank">
                        <img alt="Microsoft Azure AI" src="./assets/logos/microsoft.png" width="120">
                    </a>
                </td>
                <td align="center">
                    <a href="https://www.argo.ai/" target="_blank">
                        <img alt="CMU" src="./assets/logos/argo.png" width="70">
                    </a>
                </td>
            </tr>
        </tbody>
    </table>

    <table align="center" class="links">
        <tr>
            <td align="center">
                <a href="https://nips.cc/Conferences/2022/">üåê NeurIPS 2022</a>
            </td>
            <td align="center">
                <a href="https://arxiv.org/abs/2210.10774">üìÑ Paper</a>
            </td>
            <td align="center">
                <a href="https://github.com/vlfom/RNCDL">üíª Source code</a>
            </td>
        </tr>
    </table>

    <center>
        <img src="./assets/figures/qualitative_results.webp" class="qualitative_teaser">
    </center>

    <hr>

    <center>
        <center class="abstract-title">
            Abstract
        </center>
        <div class="abstract">
            <span>
                We tackle the problem of <div class="hlight">novel class discovery, detection, and localization (NCDL)</div>. In this setting, we assume a source dataset with labels for objects of commonly observed classes. Instances of other classes need to be discovered, classified, and localized automatically based on visual similarity, without human supervision. To this end, we propose a <div class="hlight">two-stage object detection network Region-based NCDL (RNCDL)</div>, that uses a region proposal network to localize object candidates and is trained to classify each candidate, either as one of the known classes, seen in the source dataset, or one of the extended set of novel classes, with a long-tail distribution constraint on the class assignments, reflecting the natural frequency of classes in the real world. By training our detection network with this objective in an end-to-end manner, it learns to classify all region proposals for a large variety of classes, including those that are not part of the labeled object class vocabulary.
            </span>
            <div style="margin-top: 10px;">
                Our experiments conducted using <div class=" hlight">COCO</div> and <div class=" hlight">LVIS</div> datasets reveal that our method is significantly more effective compared to multi-stage pipelines that rely on traditional clustering algorithms or use pre-extracted crops. Furthermore, we demonstrate the generality of our approach by applying our method to a large-scale <div class=" hlight">Visual Genome</div> dataset, where our network successfully learns to detect various semantic classes without explicit supervision.
            </div>
        </div>
    </center>

    <hr>

    <center class="framework-overview">
        <div class="framework-overview-title">Task and framework overview</div>
        <img src="./assets/figures/teaser.webp" class="framework-teaser-img">
        <div class="framework-overview-note">
            In <div class=" hlight">novel class discovery and localization</div> task we assume given a labeled images
            pool containing annotations for <i>known</i>, frequently observed semantic classes, and unlabeled images pool
            that may contain instances of <i>novel</i> classes. Our network learns to localize and recognize common semantic
            classes, as well as categorize novel classes, for which supervision was not given.
        </div>
    </center>

    <hr>

    <center class="our-approach">
        <div class="our-approach-title">Our approach: Region-based Novel Category Discovery and Localization (RNCDL)</div>
        <img src="./assets/figures/framework_overview.webp" class="our-approach-img">
        <div class="our-approach-note">
            <div>
                <div class=" hlight">Top:</div> during supervised training phase, we train our backbone and RPN
                networks using labeled data, together with classification head and a class agnostic localization head. During
                discovery phase, we freeze all the layers of the network apart from classification head and attach and train a
                novel classification head using unlabeled data.
            </div>
            <div style="margin-top: 10px">
                <div class="hlight">Bottom:</div> during inference phase, we perform a standard R-CNN pass, using
                classification heads of both known and novel categories to predict a class assignment for each proposal.
                This can be either one of K classes, that were presented a labeled samples during the model training, or any
                novel object class that appears in the training data.
            </div>
        </div>
    </center>

    <hr>

    <center class=" results">
        <div class="results-title">Results</div>
        <div class="results-content">
            <div class="results-note">
                <div>
                    We evaluate our method on <div class=" hlight">COCO</div> and <div class=" hlight">LVIS</div> datasets, where we use random 50% of COCO dataset during the supervised phase, and the rest unlabeled images during the discovery phase. We compare our method with k-means baselines and three recent state-of-the-art methods that we adapted to our scenario. We present the results for both known (COCO) and unknown (the rest of LVIS) classes. In the manuscript, we provide additional results on the Visual Genome dataset.
                </div>
                <div style="margin-top: 10px">
                    Our method <div class=" hlight">significantly outperforms</div> <div class=" hlight">baselines and SOTA</div> methods: the previous best NCD method UNO, by reaching 4.74 higher overall mAP, and approach by Weng et al.
                </div>
            </div>
            <img src="./assets/figures/performance_sota.png" class="results-img">
        </div>
    </center>

    <hr>

    <center class="video">
        <div class="video-title">Video presentation</div>
        <div class="video-content">
            Video is to be released soon.
            <!-- <video width="854px" height="480px" controls>
                <source src="./assets/videos/xxx.mp4" type="video/mp4">
            </video> -->
        </div>
    </center>

    <hr>

    <center class="footer">
        <div class="footer-content">
            <table align="center">
                <tbody>
                    <tr>
                        <td>
                            <img class="paperpreview-img" src="./assets/figures/paper_preview.webp">
                        </td>
                        <td></td>
                        <td class="footer-text">
                                <div>Vladimir Fomenko, Ismail Elezi, Deva Ramanan, Laura Leal-Taix√©, Aljo≈°a O≈°ep</div>
                                <a href="https://vlfom.github.io/RNCDL/">Learning to Discover and Detect Objects</a>
                                <div><i>Advances in Neural Information Processing Systems 36 (NeurIPS 2022)</i></div>
                                <a href="https://arxiv.org/abs/2210.10774">üìÑ Paper</a>
                                <a href="https://github.com/vlfom/RNCDL">üíª Code</a>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
    </center>
</body>

</html>
